{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f02bf39",
   "metadata": {},
   "source": [
    "# Pyspark Handling Missing values\n",
    "\n",
    "- Dropping Columns\n",
    "- Dropping Rows\n",
    "- Various Parameter In Dropping funtionalities\n",
    "- Handling Missing values by Mean, Median, Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a91e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa46bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7126f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(r\"C:\\Users\\JEYA KUMAR R\\Downloads\\DataSet\\Book1.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c00ee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  Name| Age| Exp|Salary|\n",
      "+------+----+----+------+\n",
      "|  Jeya|  43|  10| 30000|\n",
      "| Kumar|  44|   8| 25000|\n",
      "|  Raja|  21|   3| 10000|\n",
      "|Kannan|  33|   4| 15000|\n",
      "|  John|  56|   1| 22000|\n",
      "| peter|  32|   2| 29000|\n",
      "|kamesh|null|null| 22200|\n",
      "|  null|  34|  10| 40000|\n",
      "|  null|  36|null|  null|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c8181e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "| Age| Exp|Salary|\n",
      "+----+----+------+\n",
      "|  43|  10| 30000|\n",
      "|  44|   8| 25000|\n",
      "|  21|   3| 10000|\n",
      "|  33|   4| 15000|\n",
      "|  56|   1| 22000|\n",
      "|  32|   2| 29000|\n",
      "|null|null| 22200|\n",
      "|  34|  10| 40000|\n",
      "|  36|null|  null|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns\n",
    "\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abf80389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  Name| Age| Exp|Salary|\n",
      "+------+----+----+------+\n",
      "|  Jeya|  43|  10| 30000|\n",
      "| Kumar|  44|   8| 25000|\n",
      "|  Raja|  21|   3| 10000|\n",
      "|Kannan|  33|   4| 15000|\n",
      "|  John|  56|   1| 22000|\n",
      "| peter|  32|   2| 29000|\n",
      "|kamesh|null|null| 22200|\n",
      "|  null|  34|  10| 40000|\n",
      "|  null|  36|null|  null|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70535d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+------+\n",
      "|  Name|Age|Exp|Salary|\n",
      "+------+---+---+------+\n",
      "|  Jeya| 43| 10| 30000|\n",
      "| Kumar| 44|  8| 25000|\n",
      "|  Raja| 21|  3| 10000|\n",
      "|Kannan| 33|  4| 15000|\n",
      "|  John| 56|  1| 22000|\n",
      "| peter| 32|  2| 29000|\n",
      "+------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788ca7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  Name| Age| Exp|Salary|\n",
      "+------+----+----+------+\n",
      "|  Jeya|  43|  10| 30000|\n",
      "| Kumar|  44|   8| 25000|\n",
      "|  Raja|  21|   3| 10000|\n",
      "|Kannan|  33|   4| 15000|\n",
      "|  John|  56|   1| 22000|\n",
      "| peter|  32|   2| 29000|\n",
      "|kamesh|null|null| 22200|\n",
      "|  null|  34|  10| 40000|\n",
      "|  null|  36|null|  null|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# any == how\n",
    "# all option will drop the row if all values is null \n",
    "# default it will be any\n",
    "\n",
    "df_pyspark.na.drop(how = \"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbea09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  Name| Age| Exp|Salary|\n",
      "+------+----+----+------+\n",
      "|  Jeya|  43|  10| 30000|\n",
      "| Kumar|  44|   8| 25000|\n",
      "|  Raja|  21|   3| 10000|\n",
      "|Kannan|  33|   4| 15000|\n",
      "|  John|  56|   1| 22000|\n",
      "| peter|  32|   2| 29000|\n",
      "|kamesh|null|null| 22200|\n",
      "|  null|  34|  10| 40000|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# threshold\n",
    "# thresh = 2 which means atleast two non-null values should be in row\n",
    "df_pyspark.na.drop(how = \"any\", thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "833d9278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+------+\n",
      "|  Name|Age|Exp|Salary|\n",
      "+------+---+---+------+\n",
      "|  Jeya| 43| 10| 30000|\n",
      "| Kumar| 44|  8| 25000|\n",
      "|  Raja| 21|  3| 10000|\n",
      "|Kannan| 33|  4| 15000|\n",
      "|  John| 56|  1| 22000|\n",
      "| peter| 32|  2| 29000|\n",
      "|  null| 34| 10| 40000|\n",
      "+------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subset\n",
    "# subset = ['Exp'] which means null values will be dropped in Exp column only\n",
    "df_pyspark.na.drop(how = \"any\", subset = ['Exp']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e49322d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+------+\n",
      "|   Name| Age| Exp|Salary|\n",
      "+-------+----+----+------+\n",
      "|   Jeya|  43|  10| 30000|\n",
      "|  Kumar|  44|   8| 25000|\n",
      "|   Raja|  21|   3| 10000|\n",
      "| Kannan|  33|   4| 15000|\n",
      "|   John|  56|   1| 22000|\n",
      "|  peter|  32|   2| 29000|\n",
      "| kamesh|null|null| 22200|\n",
      "|Missing|  34|  10| 40000|\n",
      "|Missing|  36|null|  null|\n",
      "+-------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filling the Missing values\n",
    "\n",
    "df_pyspark.na.fill('Missing', 'Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd2f9bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  Name| Age| Exp|Salary|\n",
      "+------+----+----+------+\n",
      "|  Jeya|  43|  10| 30000|\n",
      "| Kumar|  44|   8| 25000|\n",
      "|  Raja|  21|   3| 10000|\n",
      "|Kannan|  33|   4| 15000|\n",
      "|  John|  56|   1| 22000|\n",
      "| peter|  32|   2| 29000|\n",
      "|kamesh|null|null| 22200|\n",
      "|  null|  34|  10| 40000|\n",
      "|  null|  36|null|  null|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30f3dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age', 'Exp', 'Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age', 'Exp', 'Salary']]\n",
    "    ).setStrategy(\"mean\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bde56cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|  Name| Age| Exp|Salary|Age_imputed|Exp_imputed|Salary_imputed|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|  Jeya|  43|  10| 30000|         43|         10|         30000|\n",
      "| Kumar|  44|   8| 25000|         44|          8|         25000|\n",
      "|  Raja|  21|   3| 10000|         21|          3|         10000|\n",
      "|Kannan|  33|   4| 15000|         33|          4|         15000|\n",
      "|  John|  56|   1| 22000|         56|          1|         22000|\n",
      "| peter|  32|   2| 29000|         32|          2|         29000|\n",
      "|kamesh|null|null| 22200|         37|          5|         22200|\n",
      "|  null|  34|  10| 40000|         34|         10|         40000|\n",
      "|  null|  36|null|  null|         36|          5|         24150|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef926e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age', 'Exp', 'Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age', 'Exp', 'Salary']]\n",
    "    ).setStrategy(\"median\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2c70ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|  Name| Age| Exp|Salary|Age_imputed|Exp_imputed|Salary_imputed|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|  Jeya|  43|  10| 30000|         43|         10|         30000|\n",
      "| Kumar|  44|   8| 25000|         44|          8|         25000|\n",
      "|  Raja|  21|   3| 10000|         21|          3|         10000|\n",
      "|Kannan|  33|   4| 15000|         33|          4|         15000|\n",
      "|  John|  56|   1| 22000|         56|          1|         22000|\n",
      "| peter|  32|   2| 29000|         32|          2|         29000|\n",
      "|kamesh|null|null| 22200|         34|          4|         22200|\n",
      "|  null|  34|  10| 40000|         34|         10|         40000|\n",
      "|  null|  36|null|  null|         36|          4|         22200|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d177b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age', 'Exp', 'Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age', 'Exp', 'Salary']]\n",
    "    ).setStrategy(\"mode\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc25cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|  Name| Age| Exp|Salary|Age_imputed|Exp_imputed|Salary_imputed|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "|  Jeya|  43|  10| 30000|         43|         10|         30000|\n",
      "| Kumar|  44|   8| 25000|         44|          8|         25000|\n",
      "|  Raja|  21|   3| 10000|         21|          3|         10000|\n",
      "|Kannan|  33|   4| 15000|         33|          4|         15000|\n",
      "|  John|  56|   1| 22000|         56|          1|         22000|\n",
      "| peter|  32|   2| 29000|         32|          2|         29000|\n",
      "|kamesh|null|null| 22200|         21|         10|         22200|\n",
      "|  null|  34|  10| 40000|         34|         10|         40000|\n",
      "|  null|  36|null|  null|         36|         10|         10000|\n",
      "+------+----+----+------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0c212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
